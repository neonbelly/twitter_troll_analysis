{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install twitterscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import twitterscraper\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import datetime\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from twitterscraper import query_tweets_from_user\n",
    "from twitterscraper import User\n",
    "from twitterscraper import query\n",
    "from twitterscraper import query_tweets\n",
    "from twitterscraper import query_user_info\n",
    "import logging\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verified Users Source\n",
    "\n",
    "https://twitter.com/jasonbaumgartne/status/1008959965162299394\n",
    "\n",
    "\n",
    "https://medium.com/@bansalsamarth/this-espn-analyst-comes-closest-to-what-the-median-twitter-verified-user-looks-like-c1818aafc6e7\n",
    "\n",
    "The perfect snapshot of legitimate accounts from July 2018 (around the time Five Thirty Eight released the dataset, complete list of 28k verified users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_json('TU_verified.ndjson.xz', orient='records', lines=True)\n",
    "# df.columns\n",
    "# df = df[df['lang']=='en']\n",
    "# verified_users = df['screen_name'].unique()\n",
    "# with open(\"verified_users.txt\", \"wb\") as fp:\n",
    "#     pickle.dump(verified_users, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('verified_users.txt', 'rb') as fp:\n",
    "    verified_users = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming your scraping is not done yet, this is a good way to continue your scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('verified_tweets_sample.pkl')\n",
    "df = pd.DataFrame(df)  \n",
    "users_run_already = df['screen_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(verified_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verified_users = set(verified_users) - set(users_run_already)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(verified_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Verified Users\n",
    "This process will take very long. Saving files in pkl format on users that have not been scraped yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_pkl(tweets_list):\n",
    "    tweet_rows = []\n",
    "    \n",
    "    with open('verified_tweets.pkl', 'rb') as fp:\n",
    "        old_tweets = pickle.load(fp)\n",
    "        while(len(old_tweets) < 1):\n",
    "            time.sleep(5)\n",
    "            old_tweets = pickle.load(fp)\n",
    "    fp.close()\n",
    "    \n",
    "        \n",
    "    with open('verified_tweets.pkl', 'wb') as f:\n",
    "        \n",
    "        for tweet in tweets_list:\n",
    "            tweet_rows.append({\n",
    "                'user_id': tweet.user_id,\n",
    "                'username': tweet.username,\n",
    "                'screen_name': tweet.screen_name,\n",
    "                'timestamp': tweet.timestamp,\n",
    "                'text': tweet.text,\n",
    "                'hashtags': tweet.hashtags,\n",
    "                'retweets': tweet.retweets,\n",
    "                'likes': tweet.likes,\n",
    "                'has_media': tweet.has_media,\n",
    "                'links': tweet.links,\n",
    "                'is_reply_to': tweet.is_reply_to,\n",
    "                'is_replied': tweet.is_replied,\n",
    "                'parent_tweet_id': tweet.parent_tweet_id\n",
    "            })\n",
    "        tweet_rows.extend(old_tweets)\n",
    "        pickle.dump(tweet_rows, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "logger = logging.getLogger()\n",
    "logger.disabled = True\n",
    "for ix, verified_user in enumerate(verified_users):\n",
    "    #print(verified_user)\n",
    "    tweets.extend(query_tweets_from_user(verified_user))\n",
    "    if(ix % 20 == 0):\n",
    "        write_pkl(tweets)\n",
    "        tweets = []\n",
    "        \n",
    "write_pkl(tweets)\n",
    "logger.disabled = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
